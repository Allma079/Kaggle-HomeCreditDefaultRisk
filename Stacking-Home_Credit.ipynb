{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking Neural Networks and Ensemble Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parts of this kernel have been inspired from this [kernel](https://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python). \n",
    "Every Out-Of-Fold data-set is saved to be fed into the LightGBM Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.core import Flatten\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras import backend as K\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Going to use these 5 base models for the stacking\n",
    "from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n",
    "                              GradientBoostingClassifier, ExtraTreesClassifier)\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/full_features_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.set_index('SK_ID_CURR',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = pd.get_dummies(df)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = app[app['TARGET'].notnull()].copy(), app[app['TARGET'].isnull()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Dealing with NaN values which cannot be handled by NN's or sklearn ensemble algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.replace(np.inf,0,inplace=True)\n",
    "train.replace(-np.inf,0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.replace(np.inf,0,inplace=True)\n",
    "test.replace(-np.inf,0,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Out of Fold Function and SklearnHelper Classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntrain = train.shape[0]\n",
    "ntest = test.shape[0]\n",
    "SEED = 0 # for reproducibility\n",
    "NFOLDS = 3 # set folds for out-of-fold prediction\n",
    "kf = KFold(ntrain, n_folds= NFOLDS, random_state=SEED)\n",
    "\n",
    "# Class to extend the Sklearn classifier\n",
    "class SklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict_proba(x)[:,1]\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)\n",
    "    \n",
    "# Class to extend XGboost classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to extend the Sklearn classifier\n",
    "class FNNSklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        #params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        x_train = x_train.reshape(x_train.shape[0],x_train.shape[1])\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = x.reshape(x.shape[0],x.shape[1])\n",
    "        return self.clf.predict_proba(x)[:,1]\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        x = x.reshape(x.shape[0],x.shape[1])\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)\n",
    "    \n",
    "# Class to extend XGboost classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNSklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        #params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        x_train = x_train.reshape(x_train.shape[0],x_train.shape[1],1)\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        x = x.reshape(x.shape[0],x.shape[1],1)\n",
    "        return self.clf.predict_proba(x)[:,1]\n",
    "    \n",
    "    def fit(self,x,y):\n",
    "        x = x.reshape(x.shape[0],x.shape[1],1)\n",
    "        return self.clf.fit(x,y)\n",
    "    \n",
    "    def feature_importances(self,x,y):\n",
    "        print(self.clf.fit(x,y).feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, x_train, y_train, x_test):\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((NFOLDS, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = x_train[train_index]\n",
    "        y_tr = y_train[train_index]\n",
    "        x_te = x_train[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr)\n",
    "        print('Training {} done.'.format(i+1))\n",
    "        oof_train[test_index] = clf.predict(x_te).ravel()\n",
    "        oof_test_skf[i, :] = clf.predict(x_test).ravel()\n",
    "        print('Fold {} done.'.format(i+1))\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Numpy arrays of train, test and target ( Survived) dataframes to feed into our models\n",
    "y_train = train['TARGET'].ravel()\n",
    "train = train.drop(['TARGET'], axis=1)\n",
    "test = test.drop(['TARGET'], axis=1)\n",
    "x_train = train.values # Creates an array of the train data\n",
    "x_test = test.values # Creats an array of the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classifiers and Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators': 1000,\n",
    "     'warm_start': True, \n",
    "     #'max_features': 0.2,\n",
    "    'max_depth': 6,\n",
    "    'min_samples_leaf': 2,\n",
    "    'max_features' : 'sqrt',\n",
    "    'verbose': 111,\n",
    "    'class_weight' : 'balanced'\n",
    "}\n",
    "\n",
    "# Extra Trees Parameters\n",
    "et_params = {\n",
    "    'n_jobs': -1,\n",
    "    'n_estimators':1000,\n",
    "    #'max_features': 0.5,\n",
    "    'max_depth': 8,\n",
    "    'min_samples_leaf': 2,\n",
    "    'verbose': 111,\n",
    "    'class_weight' : 'balanced'\n",
    "}\n",
    "nix_params= {\n",
    "    'n_estimators': 2000,\n",
    "    'max_depth' : 4,\n",
    "    'min_child_weight' : 2,\n",
    "    'gamma' : 0.9,\n",
    "    'subsample' : 0.8,\n",
    "    'colsample_bytree' : 0.8,\n",
    "    'objective' : 'binary:logistic',\n",
    "    'nthread': -1,\n",
    "    'scale_pos_weight' : 1,\n",
    "    'verbose': 200\n",
    "}\n",
    "\n",
    "gbm_params= {\n",
    "    'objective':'binary:logistic',\n",
    "    'learning_rate': 0.00764,\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 5,\n",
    "    'verbose': 200,\n",
    "    'silent' : False,\n",
    "    'subsample': 0.6,\n",
    "    'colsample_bytree': 0.7,\n",
    "    #'n_estimators': 2673,\n",
    "    'n_estimators': 1000,\n",
    "    'gamma':0.4,\n",
    "    'nthread': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\n",
    "et = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\n",
    "gbm = SklearnHelper(clf=xgb.XGBClassifier, seed=SEED,params = gbm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "et_oof_train, et_oof_test = get_oof(et, x_train, y_train, x_test) # Extra Trees\n",
    "np.save('et_oof_train.npz',et_oof_train)\n",
    "np.save('et_oof_test.npz',et_oof_test)\n",
    "filename = 'et.sav'\n",
    "pickle.dump(et, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_oof_train, rf_oof_test = get_oof(rf,x_train, y_train, x_test) # Random Forest\n",
    "np.save('rf_oof_train.npz',rf_oof_train)\n",
    "np.save('rf_oof_test.npz',rf_oof_test)\n",
    "filename = 'rf.sav'\n",
    "pickle.dump(rf, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtrain = gbtrain.drop(['TARGET'], axis=1)\n",
    "gbtest = gbtest.drop(['TARGET'], axis=1)\n",
    "xgb_train = gbtrain.values\n",
    "xgb_test = gbtest.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbtrain, gbtest = app[app['TARGET'].notnull()].copy(), app[app['TARGET'].isnull()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_oof_train, gbm_oof_test = get_oof(gbm, xgb_train, y_train, xgb_test)\n",
    "np.save('gbm_oof_train.npz',gbm_oof_train)\n",
    "np.save('gbm_oof_test.npz',gbm_oof_test)\n",
    "filename = 'gbm.sav'\n",
    "pickle.dump(gbm, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of several different neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcnn_model(num_classes = 2,input_shape = None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128,input_shape=input_shape,kernel_regularizer=regularizers.l2(0.3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(128,kernel_regularizer=regularizers.l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(64))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "fcnn_params={\n",
    "    'build_fn': fcnn_model,\n",
    "    'epochs': 10,\n",
    "    'batch_size' : 32,\n",
    "    'verbose' : 1,\n",
    "    'input_shape': (train.shape[1],)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(num_classes = 2,input_shape = None):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(32,5,strides = 1,input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(32,5,strides = 1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_params={\n",
    "    'build_fn': cnn_model,\n",
    "    'epochs': 10,\n",
    "    'batch_size' : 256,\n",
    "    'verbose' : 1,\n",
    "    'input_shape': (train.shape[1],1)\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnn = FNNSklearnHelper(clf=KerasClassifier,seed=SEED, params=fcnn_params)\n",
    "cnn = CNNSklearnHelper(clf=KerasClassifier,seed=SEED, params=cnn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fcnn_oof_train, fcnn_oof_test = get_oof(fcnn,x_train, y_train, x_test)\n",
    "np.save('fcnn_oof_train.npz',fcnn_oof_train)\n",
    "np.save('fcnn_oof_test.npz',fcnn_oof_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_oof_train, cnn_oof_test = get_oof(cnn,x_train, y_train, x_test)\n",
    "np.save('cnn_oof_train.npz',cnn_oof_train)\n",
    "np.save('cnn_oof_test.npz',cnn_oof_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have an unbalanced dataset we balance the weights out for some models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(y_train),\n",
    "                                                 y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_params_2={\n",
    "    'build_fn': cnn_model,\n",
    "    'epochs': 10,\n",
    "    'batch_size' : 256,\n",
    "    'verbose' : 1,\n",
    "    'input_shape': (train.shape[1],1),\n",
    "    'class_weight': class_weights\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn2 = CNNSklearnHelper(clf=KerasClassifier,seed=SEED, params=cnn_params_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn2_oof_train, cnn2_oof_test = get_oof(cnn2,x_train, y_train, x_test)\n",
    "np.save('cnn2_oof_train.npz',cnn2_oof_train)\n",
    "np.save('cnn2_oof_test.npz',cnn2_oof_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnn_params_2={\n",
    "    'build_fn': fcnn_model,\n",
    "    'epochs': 15,\n",
    "    'batch_size' : 256,\n",
    "    'verbose' : 1,\n",
    "    'input_shape': (train.shape[1],),\n",
    "    'class_weight': class_weights\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnn2 = FNNSklearnHelper(clf=KerasClassifier,seed=SEED, params=fcnn_params_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fcnn2_oof_train, fcnn2_oof_test = get_oof(fcnn2,x_train, y_train, x_test)\n",
    "np.save('fcnn2_oof_train.npz',fcnn2_oof_train)\n",
    "np.save('fcnn2_oof_test.npz',fcnn2_oof_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model2(num_classes = 2,input_shape = None):\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(128,5,strides = 1,input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(128,5,strides = 1))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_params_3={\n",
    "    'build_fn': cnn_model2,\n",
    "    'epochs': 15,\n",
    "    'batch_size' : 1024,\n",
    "    'verbose' : 1,\n",
    "    'input_shape': (train.shape[1],1),\n",
    "    'class_weight': class_weights\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn3 = CNNSklearnHelper(clf=KerasClassifier,seed=SEED, params=cnn_params_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn3_oof_train, cnn3_oof_test = get_oof(cnn3,x_train, y_train, x_test)\n",
    "np.save('cnn3_oof_train.npz',cnn3_oof_train)\n",
    "np.save('cnn3_oof_test.npz',cnn3_oof_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcnn_model2(num_classes = 2,input_shape = None):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128,input_shape=input_shape,kernel_regularizer=regularizers.l2(0.3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(1024,kernel_regularizer=regularizers.l2(0.1)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnn_params_3={\n",
    "    'build_fn': fcnn_model2,\n",
    "    'epochs': 15,\n",
    "    'batch_size' : 1024,\n",
    "    'verbose' : 1,\n",
    "    'input_shape': (train.shape[1],),\n",
    "    'class_weight': class_weights\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnn3 = FNNSklearnHelper(clf=KerasClassifier,seed=SEED, params=fcnn_params_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnn3_oof_train, fcnn3_oof_test = get_oof(fcnn3,x_train, y_train, x_test)\n",
    "np.save('fcnn3_oof_train.npz',fcnn3_oof_train)\n",
    "np.save('fcnn3_oof_test.npz',fcnn3_oof_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LightGBM Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 First Level LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "class LGBSklearnHelper(object):\n",
    "    def __init__(self, clf, seed=0, params=None):\n",
    "        params['random_state'] = seed\n",
    "        self.params = params\n",
    "        #self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        dtrain = lgb.Dataset(x_train, label = y_train)\n",
    "        self.clf = lgb.train(self.params, dtrain)\n",
    "        #self.clf.train(dtrain)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_hyp = {'is_unbalance': True, \n",
    "              'n_estimators': 2673, \n",
    "              'num_leaves': 77, \n",
    "              'learning_rate': 0.00764, \n",
    "              'min_child_samples': 460, \n",
    "              'boosting_type': 'gbdt', \n",
    "              'subsample_for_bin': 240000, \n",
    "              'reg_lambda': 0.20, \n",
    "              'reg_alpha': 0.88, \n",
    "              'subsample': 0.95, \n",
    "              'colsample_bytree': 0.7,\n",
    "              'verbose':200,\n",
    "              'objective':'binary'\n",
    "             }\n",
    "lgbm = LGBSklearnHelper(clf=None, seed=SEED,params = random_hyp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice: Somehow the LightGBM algorithm doesn't retrain from scratch and needs much less time for the second and third fold training which is wrong. Needs to be debugged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_oof_train, lgbm_oof_test = get_oof(lgbm, xgb_train, y_train, xgb_test)\n",
    "np.save('lgbm_oof_train.npz',lgbm_oof_train)\n",
    "np.save('lgbm_oof_test.npz',lgbm_oof_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Second Level LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcnn_oof_train = np.load('fcnn_oof_train.npz.npy')\n",
    "fcnn_oof_test = np.load('fcnn_oof_test.npz.npy')\n",
    "fcnn2_oof_train = np.load('fcnn2_oof_train.npz.npy')\n",
    "fcnn2_oof_test = np.load('fcnn2_oof_test.npz.npy')\n",
    "fcnn3_oof_train = np.load('fcnn3_oof_train.npz.npy')\n",
    "fcnn3_oof_test = np.load('fcnn3_oof_test.npz.npy')\n",
    "cnn_oof_train = np.load('cnn_oof_train.npz.npy')\n",
    "cnn_oof_test = np.load('cnn_oof_test.npz.npy')\n",
    "cnn2_oof_train = np.load('cnn2_oof_train.npz.npy')\n",
    "cnn2_oof_test = np.load('cnn2_oof_test.npz.npy')\n",
    "cnn3_oof_train = np.load('cnn3_oof_train.npz.npy')\n",
    "cnn3_oof_test = np.load('cnn3_oof_test.npz.npy')\n",
    "et_oof_train = np.load('et_oof_train.npz.npy')\n",
    "et_oof_test = np.load('et_oof_test.npz.npy')\n",
    "rf_oof_train = np.load('rf_oof_train.npz.npy')\n",
    "rf_oof_test = np.load('rf_oof_test.npz.npy')\n",
    "gbm_oof_train = np.load('gbm_oof_train.npz.npy')\n",
    "gbm_oof_test = np.load('gbm_oof_test.npz.npy')\n",
    "lgbm_oof_train = np.load('lgbm_oof_train.npz.npy')\n",
    "lgbm_oof_test = np.load('lgbm_oof_test.npz.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_stacked = np.concatenate(( et_oof_train, rf_oof_train, cnn_oof_train, \n",
    "                                  gbm_oof_train,lgbm_oof_train, fcnn_oof_train,fcnn2_oof_train,\n",
    "                                  fcnn3_oof_train,cnn2_oof_train,cnn3_oof_train), axis=1)\n",
    "x_test_stacked = np.concatenate(( et_oof_test, rf_oof_test, cnn_oof_test, \n",
    "                                  gbm_oof_test,lgbm_oof_test, fcnn_oof_test,fcnn2_oof_test,\n",
    "                                  fcnn3_oof_test,cnn2_oof_test,cnn3_oof_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lgb = lgb.Dataset(x_train_stacked,label = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.train(random_hyp,train_lgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(x_test_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({'SK_ID_CURR': list(test.index),\n",
    "                            'TARGET': preds})\n",
    "submission.to_csv('submission_manual.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
